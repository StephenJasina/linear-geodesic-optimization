We want a smooth transition between manifolds generated by snapshots.
* Naively, we just generate manifolds for each snapshot and then linearly interpolate between them (using the fact that the surfaces are height functions). This does produce an animation, but severe changes between the snapshots will lead to near-discontinuities in the movement.
* Similarly, we can interpolate between the objective functions used to generate the manifolds. This again suffers from the same issue mentioned above. Furthermore, implementing this approach reveals that smoothly interpolating between objective functions does not necessarily smoothly interpolate between manifolds. This is especially bad because we should expect the movement at any point to be "approximately monotonic," as opposed to wiggling vertically a lot.

These observations point to the idea that we need to enforce smoothness during the optimization stage. The main issue here is that our current optimization process does not produce unique outputs. Most obviously, simply inverting the z-coordinates of every point yields the same loss. As a result, we are not guaranteed that nearby loss functions (in terms of target curvatures) will yield nearby manifolds (in terms of positions). There are a few ways to try to approach this:
* Initialize the mesh to be the output of the previous snapshot. This is simple to implement (already basically done), but it makes manifold generation prohibitively slow. Moreover, this still doesn't guarantee our desired property. In fact, we have seen examples previously where poor hyperparameter choices have lead to valleys appearing when the initialization is a dome.
* Include a new term in the loss function that somehow measures the distance to the old manifold. Most simply, this could be implemented as l-2 distance.This has the benefit of ease of implementation, but the drawback of requiring the choice of another hyperparameter. Similar to the above, this strategy incurs a steep penalty to runtime. It also suffers from being weak in a theoretical sense.

	* Maybe try starting with step 1 initialization. Or maybe reinitialize every so often
* Redefine the objective function so that the optimal manifold is unique. This would be the ideal solution, but I haven't been able to come up with a way to do so. This is something we have essentially discussed before (e.g., when talking about how our current objective function has a symmetry issue with 0 initialization) and were unsuccessful in figuring out.
* Another possibility is to simultaneously optimize across all snapshots. This is inspired by the ideas in the paper "Network Anomography." This is not particularly scalable with the currently implementation. It also would likely require some additional hyperparameter tuning, and seems like it would be rather complex.