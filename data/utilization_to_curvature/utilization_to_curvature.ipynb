{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b70bc9",
   "metadata": {},
   "source": [
    "# Tomography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b7b99",
   "metadata": {},
   "source": [
    "First, some imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import itertools\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import optimize, sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14164265",
   "metadata": {},
   "source": [
    "These are paths to the data files. We need two things:\n",
    "\n",
    "* Probes: This is a CSV file with headers `id` (string), `latitude` (float), and `longitude` (float).\n",
    "* Links: This is a CSV file with headers `source_id` (string), `target_id` (string), `throughput` (float). The two id fields need to be found in the probes file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d649c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_probes = pathlib.PurePath('probes.csv')\n",
    "filepath_links = pathlib.PurePath('links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9c1146",
   "metadata": {},
   "source": [
    "With these data files, we can generate a NetworkX graph. First, though, we need a few utility functions for dealing with geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8afdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mercator(longitude = None, latitude = None):\n",
    "    \"\"\"\n",
    "    Given a longitude in [-180, 180] and a latitude in [-90, 90], return\n",
    "    an (x, y) pair representing the location on a Mercator projection.\n",
    "    Assuming the latitude is no larger/smaller than +/- 85\n",
    "    (approximately), the pair will lie in [-0.5, 0.5]^2.\n",
    "    \"\"\"\n",
    "    x = longitude / 360. if longitude is not None else None\n",
    "    y = np.log(np.tan(np.pi / 4. + latitude * np.pi / 360.)) / (2. * np.pi) if latitude is not None else None\n",
    "\n",
    "    x = float(x)\n",
    "    y = float(y)\n",
    "\n",
    "    if x is None:\n",
    "        return y\n",
    "    if y is None:\n",
    "        return x\n",
    "    return (x, y)\n",
    "\n",
    "def get_sphere_point(latlong):\n",
    "    \"\"\"Convert spherical coordinates to Cartesian coordinates.\"\"\"\n",
    "    latlong = np.array(latlong) * np.pi / 180.\n",
    "    return np.array([\n",
    "        np.cos(latlong[1]) * np.cos(latlong[0]),\n",
    "        np.sin(latlong[1]) * np.cos(latlong[0]),\n",
    "        np.sin(latlong[0])\n",
    "    ])\n",
    "\n",
    "def get_spherical_distance(a, b):\n",
    "    \"\"\"Find the distance on the unit sphere between two unit vectors.\"\"\"\n",
    "    # The value of (a @ b) is clamped between -1 and 1 to avoid issues\n",
    "    # with floating point\n",
    "    if np.all(a == b):\n",
    "        return np.float64(0.)\n",
    "    return np.arccos(min(max(a @ b, -1.), 1.))\n",
    "\n",
    "def get_GCL(latlong_a, latlong_b):\n",
    "    \"\"\"Find the great circle latency between two points on Earth in ms.\"\"\"\n",
    "    # The following are in meters and meters per second\n",
    "    circumference_earth = 40075016.68557849\n",
    "    radius_earth = circumference_earth / (2. * np.pi)\n",
    "    c = 299792458.\n",
    "\n",
    "    # Convert spherical coordinates to Cartesian coordinates\n",
    "    p_a = get_sphere_point(latlong_a)\n",
    "    p_b = get_sphere_point(latlong_b)\n",
    "\n",
    "    # Special case for slightly improved numerical stability\n",
    "    if np.all(p_a == p_b):\n",
    "        return 0.\n",
    "\n",
    "    # Compute the latency, which is the travel time at the rate of two\n",
    "    # thirds the speed of light\n",
    "    return 2. * 1000. * get_spherical_distance(p_a, p_b) * radius_earth \\\n",
    "        / (2. * c / 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c856c5",
   "metadata": {},
   "source": [
    "We also need a function to deal with co-located nodes. This is not important for our tomography computations, but it will matter for the Ricci curvature later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce04eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_graph(graph: nx.DiGraph):\n",
    "    # Cluster by co-location\n",
    "    clusters = collections.defaultdict(list)\n",
    "    for node, data_node in graph.nodes(data=True):\n",
    "        latitude = data_node['latitude']\n",
    "        longitude = data_node['longitude']\n",
    "        clusters[(latitude, longitude)].append(node)\n",
    "\n",
    "    # Make mappings from cluster representatives to constituents and\n",
    "    # vice versa\n",
    "    representative_to_constituents = {\n",
    "        min(cluster): cluster\n",
    "        for cluster in clusters.values()\n",
    "    }\n",
    "    constituent_to_representative = {\n",
    "        constituent: representative\n",
    "        for representative, constituents in representative_to_constituents.items()\n",
    "        for constituent in constituents\n",
    "    }\n",
    "\n",
    "    graph_new = nx.DiGraph()\n",
    "    for representative in representative_to_constituents.keys():\n",
    "        graph_new.add_node(\n",
    "            representative,\n",
    "            latitude=graph.nodes[representative]['latitude'],\n",
    "            longitude=graph.nodes[representative]['longitude']\n",
    "        )\n",
    "\n",
    "    for u, v, data_u_v in graph.edges(data=True):\n",
    "        representative_u = constituent_to_representative[u]\n",
    "        representative_v = constituent_to_representative[v]\n",
    "\n",
    "        if representative_u == representative_v:\n",
    "            continue\n",
    "\n",
    "        if (representative_u, representative_v) in graph_new.edges:\n",
    "            # Aggregate with existing data\n",
    "            graph_new.edges[representative_u, representative_v]['latency'] \\\n",
    "                = min(\n",
    "                    graph_new.edges[representative_u, representative_v]['latency'],\n",
    "                    data_u_v['latency']\n",
    "                )\n",
    "            graph_new.edges[representative_u, representative_v]['throughput'] \\\n",
    "                = graph_new.edges[representative_u, representative_v]['throughput'] \\\n",
    "                    + data_u_v['latency']\n",
    "        else:\n",
    "            graph_new.add_edge(\n",
    "                representative_u, representative_v,\n",
    "                latency=data_u_v['latency'],\n",
    "                throughput=data_u_v['throughput']\n",
    "            )\n",
    "\n",
    "    return graph_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6956f8",
   "metadata": {},
   "source": [
    "Now we make the graph, as well as save some supplementary information. Importantly, we need an association between graph edges and indices. We also bookkeep the amount of data flowing into and out of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f51a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.DiGraph()\n",
    "with open(filepath_probes, 'r') as file_probes:\n",
    "    reader = csv.DictReader(file_probes)\n",
    "    for index, row in enumerate(reader):\n",
    "        graph.add_node(\n",
    "            row['id'],\n",
    "            latitude=float(row['latitude']),\n",
    "            longitude=float(row['longitude'])\n",
    "        )\n",
    "\n",
    "with open(filepath_links, 'r') as file_links:\n",
    "    reader = csv.DictReader(file_links)\n",
    "    for row in reader:\n",
    "        id_source = row['source_id']\n",
    "        id_target = row['target_id']\n",
    "\n",
    "        node_source = graph.nodes[id_source]\n",
    "        node_target = graph.nodes[id_target]\n",
    "\n",
    "        graph.add_edge(\n",
    "            row['source_id'], row['target_id'],\n",
    "            throughput=float(row['throughput']),\n",
    "            latency=get_GCL(\n",
    "                (node_source['latitude'], node_source['longitude']),\n",
    "                (node_target['latitude'], node_target['longitude']),\n",
    "            )\n",
    "        )\n",
    "\n",
    "graph = cluster_graph(graph)\n",
    "\n",
    "index_to_link_id = []\n",
    "link_id_to_index = {}\n",
    "traffic_out_per_node = collections.defaultdict(float)\n",
    "traffic_in_per_node = collections.defaultdict(float)\n",
    "traffic_total = 0.\n",
    "for index, (id_source, id_target, throughput) in enumerate(graph.edges.data('throughput')):\n",
    "    index_to_link_id.append((id_source, id_target))\n",
    "    link_id_to_index[(id_source, id_target)] = index\n",
    "\n",
    "    traffic_out_per_node[id_source] += throughput\n",
    "    traffic_in_per_node[id_target] += throughput\n",
    "    traffic_total += throughput"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0acaf28",
   "metadata": {},
   "source": [
    "Now we start solving a tomography problem. Our strategy is to follow the ideas from [An information-theoretic approach to traffic matrix estimation](https://dl.acm.org/doi/10.1145/863955.863990).\n",
    "\n",
    "Our first goal is to determine how much traffic is going between each source-destination pair in our network. For simplicity, assume that each of these pairs define a single route (say, by shortest-path routing). This should be a pretty easy assumption to adapt around in the future if we want, but it makes this demonstration a bit easier.\n",
    "\n",
    "Let $A$ be our routing matrix. $A$ is $n \\times p$, where $n$ is the size of the observed data (link utilizations), and $p$ is the number of possible routes (number of source-destination pairs*). We also define $y$, an $n$-vector containing the link utilizations**. We set $A_{i, j}$ to $1$ if the $i$ th link is used in the $j$ th route, and $0$ otherwise. Note that $A$ is sparse.\n",
    "\n",
    "Our goal is to solve the (underconstrained) linear problem $y = Ax$ for the $p$-vector $x$.\n",
    "\n",
    "*We actually don't need every possible pair. There are some obvious pairs that will have no associated traffic. For example, if a node has no outgoing (incoming) traffic, then any route with it as a source (destination) will have no traffic. So we can remove the corresponding columns of $A$ and elements of $x$.\n",
    "\n",
    "**Here and elsewhere, we actually scale $y$ and $x$ by the total traffic in the system. It turns out that this makes it easier on the optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07a8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a scaling of y by the total traffic in the system\n",
    "traffic_counts = np.array([\n",
    "    graph.edges[source_id, target_id]['throughput']\n",
    "    for source_id, target_id in index_to_link_id\n",
    "])\n",
    "\n",
    "# Determine the ordering of the columns of A, ignoring\n",
    "# source-destination pairs with no possible traffic\n",
    "sources, destinations = zip(*[\n",
    "    (source, destination)\n",
    "    for source in graph.nodes\n",
    "    for destination in graph.nodes\n",
    "    if (\n",
    "        source != destination\n",
    "        and traffic_out_per_node[source] > 0.\n",
    "        and traffic_in_per_node[destination] > 0.\n",
    "    )\n",
    "])\n",
    "\n",
    "# Also have a mapping to go from source-destination pairs to indices\n",
    "source_destination_to_index = {\n",
    "    (source, destination): index\n",
    "    for index, (source, destination) in enumerate(zip(sources, destinations))\n",
    "}\n",
    "\n",
    "# Compute routes between each source-destination pair. Assume shortest\n",
    "# path routing for this example.\n",
    "routes = {\n",
    "    source: nx.single_source_dijkstra_path(graph, source, weight='latency')\n",
    "    for source in graph.nodes\n",
    "}\n",
    "\n",
    "# Create the (sparse) traffic matrix and its transpose\n",
    "traffic_matrix_data = []\n",
    "traffic_matrix_row_ind = []\n",
    "traffic_matrix_col_ind = []\n",
    "for index, (source, destination) in enumerate(zip(sources, destinations)):\n",
    "    route = routes[source][destination]\n",
    "    for link_id in itertools.pairwise(route):\n",
    "        traffic_matrix_data.append(1)\n",
    "        traffic_matrix_row_ind.append(link_id_to_index[link_id])\n",
    "        traffic_matrix_col_ind.append(index)\n",
    "\n",
    "traffic_matrix = sparse.csr_matrix(\n",
    "    (traffic_matrix_data, (traffic_matrix_row_ind, traffic_matrix_col_ind)),\n",
    "    shape=(len(index_to_link_id), len(sources))\n",
    ")\n",
    "traffic_matrix_transpose = sparse.csr_matrix(\n",
    "    (traffic_matrix_data, (traffic_matrix_col_ind, traffic_matrix_row_ind)),\n",
    "    shape=(len(sources), len(index_to_link_id))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35106698",
   "metadata": {},
   "source": [
    "Ideally, we want to find $x$ such that $y = Ax$ that minimizes some objective function $J(x)$. We do this by solving the \"soft\" problem of minimizing $f(x) = \\|y - Ax\\|_2^2 + \\lambda J(x)$ across choices of $x$, where $\\lambda$ is a tunable hyperparameter.\n",
    "\n",
    "For our choice of $J$, we use the mutual information between $S$ and $D$, where $S$ is a random variable representing a unit of traffic's source, and $D$ represents its destination.\n",
    "\n",
    "We define $\\mathcal{L}(x)$ and its gradient $\\nabla \\mathcal{L}(x)$ below. It turns out that a completely accurate implementation is somewhat complex (provided with `loss_alt` and `dif_loss_alt`). We instead use the approximations given by `loss` and `dif_loss`. Some experimentation reveals that the outputs are comparable, but `loss` allows for a significantly faster output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0768b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(xs, lam=0.01):\n",
    "    errors = traffic_counts - traffic_total * (traffic_matrix @ xs)\n",
    "    accuracy = errors @ errors\n",
    "\n",
    "    penalty = 0.\n",
    "    if lam != 0.:\n",
    "        for x, source, destination in zip(xs, sources, destinations):\n",
    "            n_s = traffic_out_per_node[source]\n",
    "            n_d = traffic_in_per_node[destination]\n",
    "            if n_s > 0. and n_d > 0. and x != 0.:\n",
    "                penalty += x * np.log2(x * traffic_total**2 / (n_s * n_d))\n",
    "\n",
    "    if np.isinf(lam):\n",
    "        return penalty\n",
    "\n",
    "    return accuracy / traffic_total**2 + lam**2 * penalty\n",
    "\n",
    "def dif_loss(xs, lam=0.01):\n",
    "    errors = traffic_counts - traffic_total * (traffic_matrix @ xs)\n",
    "    dif_accuracy = -2 * traffic_total * traffic_matrix_transpose @ errors\n",
    "\n",
    "    if lam != 0.:\n",
    "        dif_penalty = np.array([\n",
    "            np.log2(x * traffic_total**2 / (n_s * n_d)) + 1 / np.log(2)\n",
    "            if n_s > 0. and n_d > 0. else 0.\n",
    "            for x, source, destination in zip(xs, sources, destinations)\n",
    "            for n_s in (traffic_out_per_node[source],)\n",
    "            for n_d in (traffic_in_per_node[destination],)\n",
    "        ])\n",
    "    else:\n",
    "        dif_penalty = np.zeros(xs.shape)\n",
    "\n",
    "    return dif_accuracy / traffic_total**2 + lam**2 * dif_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dccccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_alt(xs, lam=0.01):\n",
    "    p_s = collections.defaultdict(float)\n",
    "    p_d = collections.defaultdict(float)\n",
    "    x_sum = 0.\n",
    "    for x, source, destination in zip(xs, sources, destinations):\n",
    "        p_s[source] += x\n",
    "        p_d[destination] += x\n",
    "        x_sum += x\n",
    "\n",
    "    errors = (traffic_counts / traffic_total) - traffic_matrix @ xs\n",
    "    accuracy = errors @ errors\n",
    "\n",
    "    penalty = 0.\n",
    "    if lam != 0.:\n",
    "        for x, source, destination in zip(xs, sources, destinations):\n",
    "            p_s_source = p_s[source]\n",
    "            p_d_destination = p_d[destination]\n",
    "            if p_s_source > 0. and p_d_destination > 0. and x != 0.:\n",
    "                penalty += (x / x_sum) * np.log2(x * x_sum / (p_s_source * p_d_destination))\n",
    "\n",
    "    if np.isinf(lam):\n",
    "        return penalty\n",
    "\n",
    "    return accuracy + lam**2 * penalty\n",
    "\n",
    "def dif_loss_alt(xs, lam=0.01):\n",
    "    p_s = collections.defaultdict(float)\n",
    "    p_d = collections.defaultdict(float)\n",
    "    x_sum = 0.\n",
    "    source_to_indices = collections.defaultdict(set)\n",
    "    destination_to_indices = collections.defaultdict(set)\n",
    "    for index, (x, source, destination) in enumerate(zip(xs, sources, destinations)):\n",
    "        p_s[source] += x\n",
    "        p_d[destination] += x\n",
    "        x_sum += x\n",
    "        source_to_indices[source].add(index)\n",
    "        destination_to_indices[destination].add(index)\n",
    "\n",
    "    errors = (traffic_counts / traffic_total) - traffic_matrix @ xs\n",
    "    dif_accuracy = -2 * traffic_matrix_transpose @ errors\n",
    "\n",
    "    dif_penalty = np.zeros(xs.shape)\n",
    "    if lam != 0.:\n",
    "        for index, (x, source, destination) in enumerate(zip(xs, sources, destinations)):\n",
    "            p_s_source = p_s[source]\n",
    "            p_d_destination = p_d[destination]\n",
    "            if p_s_source > 0. and p_d_destination > 0. and x != 0.:\n",
    "                for index_dif in range(len(xs)):\n",
    "                    if index_dif in source_to_indices[source]:\n",
    "                        if index_dif in destination_to_indices[destination]:\n",
    "                            if index == index_dif:\n",
    "                                dif_penalty[index_dif] += ((x_sum - x) / x_sum**2) * np.log2(x * x_sum / (p_s_source * p_d_destination)) \\\n",
    "                                    + ((x_sum + x) * p_s_source * p_d_destination - x * x_sum * (p_s_source + p_d_destination)) / (np.log(2) * x_sum**2 * p_s_source * p_d_destination)\n",
    "                            else:\n",
    "                                pass  # Assume no duplicate source-destination pairs\n",
    "                        else:\n",
    "                            dif_penalty[index_dif] += -(x / x_sum**2) * np.log2(x * x_sum / (p_s_source * p_d_destination)) \\\n",
    "                                + x * (p_s_source - x_sum) / (np.log(2) * x_sum**2 * p_s_source)\n",
    "                    else:\n",
    "                        if index_dif in destination_to_indices[destination]:\n",
    "                            dif_penalty[index_dif] += -(x / x_sum**2) * np.log2(x * x_sum / (p_s_source * p_d_destination)) \\\n",
    "                                + x * (p_d_destination - x_sum) / (np.log(2) * x_sum**2 * p_d_destination)\n",
    "                        else:\n",
    "                            dif_penalty[index_dif] += -(x / x_sum**2) * np.log2(x * x_sum / (p_s_source * p_d_destination)) \\\n",
    "                                + x / (np.log(2) * x_sum**2)\n",
    "\n",
    "    if np.isinf(lam):\n",
    "        return dif_penalty\n",
    "\n",
    "    return dif_accuracy + lam**2 * dif_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6037b54",
   "metadata": {},
   "source": [
    "We initialize the optimization using a simple gravity model. From there, we use a L-BFGS-B minimizer to minimize the objective function. (The source paper uses a convex optimizer in MATLAB, but SciPy's convex optimizer doesn't behave well with this objective function for some reason.) Note that the prevalence of logs in the regularization term requires setting the lower bounds on the variables to a small but positive number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d5db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gravity model\n",
    "x_0 = np.array([\n",
    "    n_s * n_d / traffic_total**2\n",
    "    for source, destination in zip(sources, destinations)\n",
    "    for n_s in (traffic_out_per_node[source],)\n",
    "    for n_d in (traffic_in_per_node[destination],)\n",
    "])\n",
    "x_0 = x_0 / sum(x_0)\n",
    "\n",
    "x, _, _ = optimize.fmin_l_bfgs_b(\n",
    "    loss, x_0, fprime=dif_loss, args=[0.01],\n",
    "    # factr=1e1, pgtol=1e-12,  # Tuning parameters for the optimizer\n",
    "    bounds=[(1e-12, 1.) for _ in x_0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a886da7f",
   "metadata": {},
   "source": [
    "Here is a histogram of the logs of the traffic estimates between each source-destination pair. Note that very few elements are set to the lower bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(x * traffic_total), bins=100)\n",
    "plt.xlabel('Log of traffic')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8323385f",
   "metadata": {},
   "source": [
    "And here is a verification that the value we computed actually reflects the measured link utilizations. A log-log plot is used for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f3a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "plot_x = [np.log(traffic_count + 1) for traffic_count in traffic_counts]\n",
    "plot_y = [np.log(traffic_estimated + 1) for traffic_estimated in (traffic_total * (traffic_matrix @ x))]\n",
    "ax.plot(plot_x, plot_y, 'b.')\n",
    "ax.set_xlabel('Actual')\n",
    "ax.set_ylabel('From Estimated Flows')\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab8c5d",
   "metadata": {},
   "source": [
    "# Curvature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6667ee",
   "metadata": {},
   "source": [
    "Now that we have (approximate) flow-level measurements between nodes, we compute the flow-associated distances between the neighborhoods of nodes (i.e., the earth mover's distance used in the computation of Ollivier-Ricci curvature).\n",
    "\n",
    "The original idea is to place a certain total amount of mass $1 - \\alpha$ on the neighbors of a node $u$ and transport it to the neighbors of an adjacent node $v$. For simplicity, let's use $\\alpha = 0$. In the future, we can make a simple adaptation to allow for other $\\alpha$ values, but it is not immediately clear how the behavior will be as we take $\\alpha \\rightarrow 1$.\n",
    "\n",
    "There are two pieces to fill in, both potentially helped by the computed flows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6573d878",
   "metadata": {},
   "source": [
    "* What do the distributions of mass look like? When considering Ricci curvature of the edge $u \\rightarrow v$, we only care about the bytes that flow from neighbors of $u$ to neighbors of $v$.\n",
    "\n",
    "  Define $f^{u \\rightarrow v}_s$ to be the amount of flow from a neighbor $s \\ne v$ of $u$ that eventually makes its way to a neighbor of $v$. In other words, we are summing flows associated to paths of the form $\\cdots \\rightarrow s \\rightarrow \\cdots \\rightarrow t \\rightarrow \\cdots$ where $t \\ne u$ is a neighbor of $v$.\n",
    "\n",
    "  Similarly, define $g^{u \\rightarrow v}_t$ to be the amount of flow passing through a neighbor of $u$ that eventually makes its way to $t \\ne u$. Here we are summing flows associated to paths of the form $\\cdots \\rightarrow s \\rightarrow \\cdots \\rightarrow t \\rightarrow \\cdots$ where $s \\ne v$ is a neighbor of $u$.\n",
    "\n",
    "  Note that we are explicitly enforcing $s \\ne v$ and $t \\ne u$ to avoid the same \"shortcutting\" problem the original Ollivier-Ricci curvature suffers from when defined on graphs.\n",
    "\n",
    "  By construction, we have\n",
    "  $$\\sum_{\\substack{s \\\\ s \\rightarrow u \\\\ s \\ne v}} f^{u \\rightarrow v}_s = \\sum_{\\substack{t \\\\ v \\rightarrow t \\\\ t \\ne u}} g^{u \\rightarrow v}_t,$$\n",
    "  as both quantities are equivalent to\n",
    "  $$M := \\sum_{\\substack{s \\\\ s \\to u \\\\ s \\ne v}} \\sum_{\\substack{t \\\\ v \\to t \\\\ t \\ne u}} \\sum_{\\substack{p \\\\ (s \\twoheadrightarrow t) \\in p}} x_p,$$\n",
    "  where $(s \\twoheadrightarrow t) \\in p$ signifies that $p$ is a path of the form $\\cdots \\rightarrow s \\rightarrow \\cdots \\rightarrow t \\rightarrow \\cdots$, and $x_p$ is amount of flow associated to $p$.\n",
    "\n",
    "  We now are prepared to define our two distributions. The source distribution simply places weights $\\frac{1}{M} f^{u \\rightarrow v}_s$ on each $s \\rightarrow u$ with $s \\ne v$; the target distribution puts mass $\\frac{1}{M} f^{u \\rightarrow v}_t$ on each $t$ with $v \\rightarrow v$ with $t \\ne u$. Our fact about $M$ shows that these distributions are truly allocations of a single unit of mass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3914ba15",
   "metadata": {},
   "source": [
    "* What does the transportation plan look like? The intuition here is that we want the solution to the transport problem to look like the actual flows we measured. The construction given in the previous bullet point allows exactly this. For a neighbor $s \\ne v$ of $u$, each unit of mass is associated to a path $\\cdots \\rightarrow s \\rightarrow \\cdots \\rightarrow t \\rightarrow \\cdots$, and we can therefore flow that mass from $s$ to $t$ along the (sub-)path. By construction, this plan does actually map the source distribution to the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadc535",
   "metadata": {},
   "source": [
    "The cost of this transportation plan is\n",
    "$$c(u, v) = \\frac{1}{M} \\sum_{\\substack{s \\\\ s \\to u \\\\ s \\ne v}} \\sum_{\\substack{t \\\\ v \\to t \\\\ t \\ne u}} \\sum_{\\substack{p \\\\ (s \\twoheadrightarrow t) \\in p}} x_p d_p(s, t),$$\n",
    "where $d_p(s, t)$ is the distance from $s$ to $t$ when following $p$. This is readily computed with the following ideas:\n",
    "\n",
    "* Iteration over the paths $p$ is done using the same routing data used to construct the traffic matrix. (This is just shortest paths in our case.)\n",
    "\n",
    "* The $x_p$ values are actually just elements of the vector $x$ normalized so that they sum to $1$, where $x$ is the vector found by the earlier tomography step. Some cancellation of fractions reveals that we can just treat $x_p$'s as the elements of $x$.\n",
    "\n",
    "* $d_p(s, t)$ can be computed by following $p$ and either counting the number of hops from $s$ to $t$ or summing measured or approximate (i.e., great circle) latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both of these are mappings from s-t pairs to their associated values\n",
    "# (denominator and numerator)\n",
    "x_sum = collections.defaultdict(float)\n",
    "xd_sum = collections.defaultdict(float)\n",
    "\n",
    "for source, routes_source in routes.items():\n",
    "    for destination, route in routes_source.items():\n",
    "        if (source, destination) not in source_destination_to_index:\n",
    "            # In this case, x_p = 0\n",
    "            continue\n",
    "\n",
    "        x_p = x[source_destination_to_index[(source, destination)]]\n",
    "        for index_s, s in enumerate(route[:-1]):\n",
    "            d_p_s_t = 0.\n",
    "            for t_previous, t in itertools.pairwise(route[index_s:]):\n",
    "                d_p_s_t += graph.edges[t_previous, t]['latency']\n",
    "\n",
    "                x_sum[(s, t)] += x_p\n",
    "                xd_sum[(s, t)] += x_p * d_p_s_t\n",
    "\n",
    "transportation_costs = {}\n",
    "for u, v in graph.edges:\n",
    "    denominator = 0.\n",
    "    numerator = 0.\n",
    "    for s in graph.predecessors(u):\n",
    "        if s == v:\n",
    "            continue\n",
    "        for t in graph.successors(v):\n",
    "            if t == u:\n",
    "                continue\n",
    "\n",
    "            denominator += x_sum[(s, t)]\n",
    "            numerator += xd_sum[(s, t)]\n",
    "    transportation_costs[(u, v)] = numerator / denominator if denominator != 0. else 3. * graph.edges[u, v]['latency']\n",
    "    if denominator == 0.:\n",
    "        print(f'Manually setting curvature of {u} -> {v} to {-2.}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8eefad",
   "metadata": {},
   "source": [
    "Now we can compute the Ricci curvatures\n",
    "$$\\kappa_{u \\rightarrow v} = 1 - \\frac{c(u, v)}{d(u, v)}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3beec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for u, v in graph.edges:\n",
    "    graph.edges[u, v]['curvature'] = 1. - transportation_costs[(u, v)] / graph.edges[u, v]['latency']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12dfcb2",
   "metadata": {},
   "source": [
    "There are many notable aspects of these curvatures. First, note that the curvature is defined on a directed graph. This turns out to be fine because the curvatures are rather symmetric across opposite edges. Note that almost all of the points in the following plot lie very close to the line $x = y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cf863",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(\n",
    "    [graph.edges[u, v]['curvature'] for (u, v) in graph.edges if (v, u) in graph.edges],\n",
    "    [graph.edges[v, u]['curvature'] for (u, v) in graph.edges if (v, u) in graph.edges],\n",
    "    'b.'\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(r'Curvature on $u \\rightarrow v$')\n",
    "ax.set_ylabel(r'Curvature on $v \\rightarrow u$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be169bc",
   "metadata": {},
   "source": [
    "Also, it is rather clear that a large proportion of the curvatures are negative. This aligns with our intuition, as the given network graph consists of critical backbone links. However, it might be challenging for our manifold optimization algorithm to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95acc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([curvature for _, _, curvature in graph.edges.data('curvature')], bins=20)\n",
    "plt.xlabel('Curvature')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ad4f4",
   "metadata": {},
   "source": [
    "Finally, there doesn't seem to be much direct correlation between throughput and curvature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ea2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [np.log(1. + graph.edges[u, v]['throughput']) for (u, v) in graph.edges],\n",
    "    [graph.edges[u, v]['curvature'] for (u, v) in graph.edges],\n",
    "    'b.'\n",
    ")\n",
    "plt.xlabel('Log of Throughput')\n",
    "plt.ylabel('Curvature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6157e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_plot(\n",
    "    graph,\n",
    "    ax = None,\n",
    "    weight_label='curvature', color_min=-2., color_max=2., colormap='RdBu'\n",
    "):\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, facecolor='#808080')\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Plot the edges\n",
    "    for u, v, d in graph.edges(data=True):\n",
    "        if (v, u) in graph.edges:\n",
    "            # Don't double-draw edges\n",
    "            if u > v:\n",
    "                continue\n",
    "\n",
    "            # For edges where an opposite exists, just draw the average\n",
    "            # value\n",
    "            weight = (d[weight_label] + graph.edges[v, u][weight_label]) / 2.\n",
    "        else:\n",
    "            weight = d[weight_label]\n",
    "        color = mpl.colormaps[colormap]((weight - color_min) / (color_max - color_min))\n",
    "\n",
    "        x_u, y_u = mercator(graph.nodes[u]['longitude'], graph.nodes[u]['latitude'])\n",
    "        x_v, y_v = mercator(graph.nodes[v]['longitude'], graph.nodes[v]['latitude'])\n",
    "        ax.plot([x_u, x_v], [y_u, y_v], color=color)\n",
    "\n",
    "    # Plot the vertices\n",
    "    for node, d in graph.nodes(data=True):\n",
    "        # If trim_vertices is set, then only plot the vertices with\n",
    "        # incident edges\n",
    "        if graph[node]:\n",
    "            x, y = mercator(graph.nodes[node]['longitude'],\n",
    "                            graph.nodes[node]['latitude'])\n",
    "            ax.plot(x, y, '.', ms=4, color='green')\n",
    "\n",
    "    return fig\n",
    "\n",
    "for u, v, throughput in graph.edges.data('throughput'):\n",
    "    graph.edges[u, v]['log_throughput'] = np.log(throughput + 1.)\n",
    "\n",
    "fig, (ax_1, ax_2) = plt.subplots(2, 1, facecolor='#808080')\n",
    "get_network_plot(\n",
    "    graph, ax_1, 'log_throughput',\n",
    "    0., max([log_throughput for _, _, log_throughput in graph.edges.data('log_throughput')]),\n",
    "    colormap='PiYG')\n",
    "get_network_plot(graph, ax_2)\n",
    "ax_1.set_title('Log Throughputs (Green = High)')\n",
    "ax_2.set_title('Curvatures (Red = Negative)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
