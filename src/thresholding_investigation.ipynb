{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import csv\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "import linear_geodesic_optimization.data.input_network as input_network\n",
    "import linear_geodesic_optimization.plot as plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by just reading in data from RIPE. This data has been pre-compiled by pulling `ping` measurements over the course of the week between midnight 2024-05-01 until midnight 2024-05-08. The ping measurements are aggregated over each consecutive hour, giving us 168 sets of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join('..', 'data', 'animation_US')\n",
    "probes_file_path = os.path.join(data_directory, 'probes.csv')\n",
    "latencies_directory = os.path.join(data_directory, 'latencies')\n",
    "clustering_distance = None\n",
    "\n",
    "# Set NetworkX graphs and latencies\n",
    "graphs = []\n",
    "gcls = {}\n",
    "latencies = []\n",
    "latencies_filenames = list(sorted(os.listdir(latencies_directory)))\n",
    "latencies_file_paths = [\n",
    "    os.path.join(latencies_directory, latencies_filename)\n",
    "    for latencies_filename in latencies_filenames\n",
    "]\n",
    "for latencies_file_path in latencies_file_paths:\n",
    "    graph = input_network.get_graph_from_paths(\n",
    "        probes_file_path, latencies_file_path,\n",
    "        clustering_distance=clustering_distance,\n",
    "        should_compute_curvatures=False\n",
    "    )\n",
    "    graphs.append(graph)\n",
    "    for u, v, d in graph.edges(data=True):\n",
    "        gcls[(u, v)] = float(d['gcl'])\n",
    "        gcls[(v, u)] = float(d['gcl'])\n",
    "    latency_dict = {}\n",
    "    for u, v, d in graph.edges(data=True):\n",
    "        latency_dict[(u, v)] = float(d['rtt'])\n",
    "        latency_dict[(v, u)] = float(d['rtt'])\n",
    "    latencies.append(latency_dict)\n",
    "\n",
    "# Mapping from probes to their cluster, in case we enable clustering\n",
    "probes_mapping = {}\n",
    "for v, d in graphs[0].nodes(data=True):\n",
    "    if 'elements' in d:\n",
    "        for u in d['elements']:\n",
    "            probes_mapping[u] = v\n",
    "    else:\n",
    "        probes_mapping[v] = v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We additionally create a combined graph by aggregating across the entire week. This will be useful in later analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latencies_combined = {}\n",
    "for latency_dict in latencies:\n",
    "    for edge, latency in latency_dict.items():\n",
    "        if edge in latencies_combined:\n",
    "            latencies_combined[edge] = min(latencies_combined[edge], latency)\n",
    "        else:\n",
    "            latencies_combined[edge] = latency\n",
    "latencies_combined = [\n",
    "    {\n",
    "        'source_id': source_id,\n",
    "        'target_id': target_id,\n",
    "        'rtt': rtt,\n",
    "    }\n",
    "    for (source_id, target_id), rtt in latencies_combined.items()\n",
    "]\n",
    "probes = [\n",
    "    {\n",
    "        'id': v,\n",
    "        'latitude': d['lat'],\n",
    "        'longitude': d['long'],\n",
    "        'city': d['city'],\n",
    "        'country': d['country'],\n",
    "    }\n",
    "    for v, d in graphs[0].nodes(data=True)\n",
    "]\n",
    "\n",
    "# Cluster this one for more analysis later\n",
    "graph_combined = input_network.get_graph(\n",
    "    probes, latencies_combined,\n",
    "    clustering_distance=clustering_distance,\n",
    "    should_compute_curvatures=False\n",
    ")\n",
    "edges_combined = set()\n",
    "latencies_combined = {}\n",
    "for u, v, d in graph_combined.edges(data=True):\n",
    "    edges_combined.add((u, v))\n",
    "    edges_combined.add((v, u))\n",
    "    latencies_combined[(u, v)] = float(d['rtt'])\n",
    "    latencies_combined[(v, u)] = float(d['rtt'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold Crossing Stability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would have some way to determine whether a link's latency crosses a given threshold (say, $\\epsilon = 10\\;\\text{ms}$). To avoid undesired oscillatory behavior on a small time scale caused by random noise, we need some way of smoothing the measurements. Let's start by looking at a single edge in particular. The edge between LA and San Jose is a decent candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_id = probes_mapping['6492']  # LA\n",
    "target_id = probes_mapping['6411']  # San Jose\n",
    "\n",
    "for i, element in enumerate(latencies):\n",
    "    if (source_id, target_id) not in element:\n",
    "        print(i)\n",
    "assert np.all([(source_id, target_id) in element for element in latencies])\n",
    "\n",
    "\n",
    "z = np.array([element[(source_id, target_id)] for element in latencies]) - gcls[(source_id, target_id)]\n",
    "plt.plot(range(len(z)), z, 'b-')\n",
    "# plt.plot([0, 167], [26.25, 26.25], 'g--')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the threshold were at $26.25\\;\\text{ms}$, then we see that a naive thresholding strategy would have the \"high\" portion drop below the level undesirably sometimes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check what the distribution of latencies looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(z[40:])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks nearly normal. This is (roughly) confirmed with a normal probability plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.probplot(z[40:], plot=plt)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's work on smoothing out the latencies.\n",
    "\n",
    "Consider a time series of points $(t_i, z_i)$, where the $z_i$'s are measured quantities. We imagine that the $z_i$'s are drawn from some probability distribution.\n",
    "\n",
    "Suppose $z_i$ is drawn from $\\mathcal{N}(\\mu_i, \\sigma_i^2)$ at time $t_i$. The log-likelihood of this observation is $$-\\frac{1}{2}\\log(2 \\pi \\sigma^2) - \\frac{(z_i - \\mu_i)^2}{2 \\sigma^2}.$$ [Some algebra and calculus](https://en.wikipedia.org/wiki/Normal_distribution#Estimation_of_parameters) show that the best estimates of the parameters for a sample of measurements are actually just the sample mean and (biased) sample variance. The resulting log-likelihood is $$-\\frac{n}{2}(\\log(2 \\pi \\widehat{\\sigma}^2) + 1).$$\n",
    "\n",
    "With this measurement in hand, we can compute the BIC for every potential model following this generative model:\n",
    "* Pick $k - 1$ changepoints from $\\{1, \\dots, n\\}$. In other words, partition $\\{1, \\dots, n\\}$ into $k$ blocks.\n",
    "* For each block, select a mean and variance. Generate each point in the partition according to a normal distribution with the corresponding parameters.\n",
    "\n",
    "This model has $2 k$ parameters. We then just minimize the BIC across all of these. For a given partition, we need only consider the maximum likelihood estimates described above. To find the best partition for a given partition, we can use a dynamic programming approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changepoints(z, max_count=None):\n",
    "    z = np.array(z)\n",
    "    n = len(z)\n",
    "    # Compute log-likelihood in O(n^2) time\n",
    "    dp_sum = np.full((n, n + 1), np.nan)\n",
    "    dp_sum_square = np.full((n, n + 1), np.nan)\n",
    "    dp_mean = np.full((n, n + 1), np.nan)\n",
    "    dp_log_likelihood = np.full((n, n + 1), np.nan)\n",
    "    for i in range(n):\n",
    "        dp_sum[i, i] = 0.\n",
    "        dp_sum_square[i, i] = 0.\n",
    "        dp_log_likelihood[i, i + 1] = 0.\n",
    "        for j in range(i + 1, n + 1):\n",
    "            dp_sum[i, j] = dp_sum[i, j - 1] + z[j - 1]\n",
    "            dp_sum_square[i, j] = dp_sum_square[i, j - 1] + z[j - 1]**2\n",
    "            dp_mean[i, j] = dp_sum[i, j] / (j - i)\n",
    "        for j in range(i + 2, n + 1):\n",
    "            variance_i_j = dp_sum_square[i, j] / (j - i) - dp_mean[i, j]**2\n",
    "            if variance_i_j <= 0:\n",
    "                dp_log_likelihood[i, j] = -np.inf\n",
    "            else:\n",
    "                dp_log_likelihood[i, j] = -((j - i) / 2) * (np.log(2 * np.pi * variance_i_j) + 1)\n",
    "    # At this point, dp_log_likelihood[i, j] is the log-likelihood of the\n",
    "    # MLE of z[i:j].\n",
    "\n",
    "    dp_bics = []\n",
    "    dp_changepoints = []\n",
    "    k_max = max_count if max_count is not None else n\n",
    "    # Element (l, i) is the best BIC score with l changepoints and i\n",
    "    # elements (that is, looking at z[:i])\n",
    "    dp_log_likelihood_partition = np.full((k_max, n + 1), np.nan)\n",
    "    dp_backtrack = np.full((k_max, n + 1), -1, dtype=int)\n",
    "    # Fill in the DP table\n",
    "    for i in range(n + 1):\n",
    "        dp_log_likelihood_partition[0, i] = dp_log_likelihood[0, i]\n",
    "    for l in range(1, k_max):\n",
    "        for i in range(l + 1, n + 1):\n",
    "            log_likelihoods = dp_log_likelihood_partition[l - 1, l : i] + np.array([\n",
    "                dp_log_likelihood[i - j, i]\n",
    "                for j in range(i - l, 0, -1)\n",
    "            ])\n",
    "            max_index = np.argmax(log_likelihoods)\n",
    "            dp_backtrack[l, i] = l + max_index\n",
    "            dp_log_likelihood_partition[l, i] = log_likelihoods[max_index]\n",
    "\n",
    "    # Iterate over the number of blocks in the partition. Note that the\n",
    "    # number of parameters in the model is twice the number of blocks\n",
    "    k_best = np.argmin([\n",
    "        2 * k * np.log(n) - 2 * dp_log_likelihood_partition[k - 1, n]  # BIC\n",
    "        for k in range(1, k_max + 1)\n",
    "    ]) + 1\n",
    "    if k_best == 1:\n",
    "        changepoints = []\n",
    "        means = [dp_mean[0, -1]]\n",
    "    else:\n",
    "        backtrack_path = [dp_backtrack[k_best - 1, n]]\n",
    "        for l in range(k_best - 2):\n",
    "            backtrack_path.append(dp_backtrack[k_best - l - 1, backtrack_path[-1]])\n",
    "\n",
    "        # Backtrack to find the changepoints\n",
    "        changepoints = list(reversed(backtrack_path))\n",
    "        means = [dp_mean[0, changepoints[0]]] + [\n",
    "            dp_mean[changepoints[i], changepoints[i + 1] - 1]\n",
    "            for i in range(len(changepoints) - 1)\n",
    "        ] + [dp_mean[changepoints[-1], n]]\n",
    "\n",
    "    return changepoints, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changepoints, means = get_changepoints(z)\n",
    "endpoints = [0] + changepoints + [len(z)]\n",
    "\n",
    "x = list(range(len(z)))\n",
    "plt.plot(x, z, 'b-')\n",
    "for i in range(len(means)):\n",
    "    plt.plot([endpoints[i] - 0.5, endpoints[i + 1] - 0.5], [means[i], means[i]], 'r-')\n",
    "# plt.plot([0, 167], [26.25, 26.25], 'g--')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this plot, we see that the smoothed latencies (in red) are more stable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next is a quick investigation about how often the changepoint detection operates on a fixed threshold (10 ms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 10\n",
    "\n",
    "count = 0\n",
    "for edge in graph_combined.edges:\n",
    "    z = []\n",
    "    for i, latency_dict in enumerate(latencies):\n",
    "        if edge in latency_dict:\n",
    "            z.append(latency_dict[edge])\n",
    "    if epsilon > np.amin(z, initial=np.inf) - gcls[edge] \\\n",
    "            and epsilon < np.amax(z, initial=-np.inf) - gcls[edge]:\n",
    "        count += 1\n",
    "\n",
    "print(\n",
    "    f'{count} edges around the threshold {epsilon} '\n",
    "    f'({100 * count / len(graph_combined.edges):0.4f}%)'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upshot is that we need to consider around 5% of the edges over the course of the week. The other edges are either always in or always out of the network graph (due to having sufficiently low or sufficiently high residuals)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Search for a Skeleton Graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would be able to automatically pick a latency threshold $\\epsilon$ automatically. The general idea would be to identify some subset of the edges of our nonthresholded network graph that are stable. We would then pick a threshold so that the \"skeleton graph\" formed by these edges is present.\n",
    "\n",
    "Let's start by investigating how the latencies of the edges vary over time. We plot out the log variances (as these follow a more understandable distribution in this case). We additionally plot the log of the variances scaled by the means of the latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = {}\n",
    "variances_normalized = {}\n",
    "for edge in gcls:\n",
    "    z = []\n",
    "    for latency_dict in latencies:\n",
    "        if edge in latency_dict:\n",
    "            z.append(latency_dict[edge])\n",
    "    variances[edge] = np.log(np.var(z))\n",
    "    variances_normalized[edge] = np.log(np.var(z) / gcls[edge]**2)\n",
    "fig, (ax1, ax2) = plt.subplots(2, dpi=200)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "ax1.set_title('Log Variances')\n",
    "ax1.hist(variances.values(), 40)\n",
    "ax1.set_xlabel(r'$\\log(\\sigma^2)$')\n",
    "ax1.set_ylabel('Count')\n",
    "ax2.set_title('Log Normalized Variances')\n",
    "ax2.hist(variances_normalized.values(), 40)\n",
    "ax2.set_xlabel(r'$\\log(\\sigma^2)$ (Normalized)')\n",
    "ax2.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also plot the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = {\n",
    "    edge: min([\n",
    "        latency_dict[edge]\n",
    "        for latency_dict in latencies\n",
    "        if edge in latency_dict\n",
    "    ]) - gcl\n",
    "    for edge, gcl in gcls.items()\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.hist(residuals.values(), 40)\n",
    "ax.set_xlabel('Residual')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we combine the above plots, and we additionally add some coloration for when links have the potential to have negative curvature (i.e. have negative curvature at some (integer) latency threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilons = list(range(int(np.ceil(max(residuals.values()))), 0, -1))\n",
    "curvatures = []\n",
    "negative_edges = collections.defaultdict(list)\n",
    "graph = copy.deepcopy(graph_combined)\n",
    "for epsilon in epsilons:\n",
    "    graph = input_network.threshold_graph(graph, epsilon)\n",
    "    if len(graph.edges) == 0:\n",
    "        curvatures.append([])\n",
    "    else:\n",
    "        graph = input_network.compute_ricci_curvatures(graph)\n",
    "        curvatures.append([\n",
    "            d['ricciCurvature']\n",
    "            for _, _, d in graph.edges(data=True)\n",
    "        ])\n",
    "        for u, v, d in graph.edges(data=True):\n",
    "            if d['ricciCurvature'] < 0.:\n",
    "                negative_edges[u, v].append((epsilon, d['ricciCurvature']))\n",
    "                negative_edges[v, u].append((epsilon, d['ricciCurvature']))\n",
    "\n",
    "    # plot.get_network_plot(graph)\n",
    "    # plt.savefig(os.path.join('figs', 'networks', f'{epsilon}.png'), dpi=200)\n",
    "    # plt.close()\n",
    "\n",
    "# These now contain the the latency thresholds and the corresponding\n",
    "# curvatures of the edges at those thresholds\n",
    "epsilons = list(reversed(epsilons))\n",
    "curvatures = list(reversed(curvatures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.scatter(\n",
    "    [residuals[edge] for edge in gcls],\n",
    "    [variances_normalized[edge] for edge in gcls],\n",
    "    s=1.,\n",
    "    c=[\n",
    "        'r' if edge in negative_edges else 'b'\n",
    "        for edge in gcls\n",
    "    ]\n",
    ")\n",
    "ax.set_xlabel('Minimal Residual')\n",
    "ax.set_ylabel(r'$\\log(\\sigma^2)$ (Normalized)')\n",
    "# fig.savefig(os.path.join('figs', 'variances.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we see that the \"stability\" of edges doesn't really correlate with the residual. Therefore, the idea of a skeleton graph is not viable. In particular, if we try to include all of the stable edges, then our $\\epsilon$ would have to be very large (so large that there would be no negative curvature edges). Conversely, small values of $\\epsilon$ include unstable edges.\n",
    "\n",
    "Additionally, being negatively (red) or positively (blue) curved doesn't imply stability."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do more analysis by separating based on the lengths of the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(gcls.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near_cuttoff = 40. * 2. / (186. * 2. / 3.)\n",
    "far_cuttoff = 500. * 2. / (186. * 2. / 3.)\n",
    "\n",
    "fig, (ax_near, ax_mid, ax_far) = plt.subplots(3, dpi=200)\n",
    "fig.subplots_adjust(hspace=0.9)\n",
    "edges_near = [\n",
    "    edge\n",
    "    for edge, gcl in gcls.items()\n",
    "    if gcl < near_cuttoff\n",
    "]\n",
    "ax_near.set_title('Near')\n",
    "ax_near.scatter(\n",
    "    [residuals[edge] for edge in edges_near],\n",
    "    [variances_normalized[edge] for edge in edges_near],\n",
    "    s=1.,\n",
    "    c=[\n",
    "        'r' if edge in negative_edges else 'b'\n",
    "        for edge in edges_near\n",
    "    ]\n",
    ")\n",
    "ax_near.set_xlabel('Minimal Residual')\n",
    "ax_near.set_ylabel(r'$\\log(\\sigma^2)$ (Normalized)')\n",
    "edges_mid = [\n",
    "    edge\n",
    "    for edge, gcl in gcls.items()\n",
    "    if gcl >= near_cuttoff and gcl < far_cuttoff\n",
    "]\n",
    "ax_mid.set_title('Mid')\n",
    "ax_mid.scatter(\n",
    "    [residuals[edge] for edge in edges_mid],\n",
    "    [variances_normalized[edge] for edge in edges_mid],\n",
    "    s=1.,\n",
    "    c=[\n",
    "        'r' if edge in negative_edges else 'b'\n",
    "        for edge in edges_mid\n",
    "    ]\n",
    ")\n",
    "ax_mid.set_xlabel('Minimal Residual')\n",
    "ax_mid.set_ylabel(r'$\\log(\\sigma^2)$ (Normalized)')\n",
    "edges_far = [\n",
    "    edge\n",
    "    for edge, gcl in gcls.items()\n",
    "    if gcl >= far_cuttoff\n",
    "]\n",
    "ax_far.set_title('Far')\n",
    "ax_far.scatter(\n",
    "    [residuals[edge] for edge in edges_far],\n",
    "    [variances_normalized[edge] for edge in edges_far],\n",
    "    s=1.,\n",
    "    c=[\n",
    "        'r' if edge in negative_edges else 'b'\n",
    "        for edge in edges_far\n",
    "    ]\n",
    ")\n",
    "ax_far.set_xlabel('Minimal Residual')\n",
    "ax_far.set_ylabel(r'$\\log(\\sigma^2)$ (Normalized)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_count = max([index for index, element in enumerate(curvatures) if np.amin(element, initial=np.inf) < 0.]) + 1\n",
    "\n",
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.boxplot(curvatures[:box_count], 0, 'kD')\n",
    "ax.set_xticks(\n",
    "    [y + 1 for y in range(box_count)],\n",
    "    labels=epsilons[:box_count])\n",
    "ax.set_xlabel('Threshold')\n",
    "ax.set_ylabel('Ricci Curvature')\n",
    "# fig.savefig(os.path.join('figs', 'boxplot.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since box plots don't necessarily give the whole picture, let's look at one particular (arbitrary) distribution of the curvatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=200)\n",
    "ax.hist(curvatures[18])\n",
    "ax.set_xlabel('Ricci Curvature')\n",
    "ax.set_ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that edges with negative curvature are rare, especially as the threshold increases.\n",
    "\n",
    "To observe as many of the potentially negative edges as possible, we need the threshold to be as large as possible. This, however, forces some of the edges to be positive."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to make a toy example be convincing, the made-up data should have as many of the same properties as we see above. Fortunately for us, the most challenging of these (distribution of the variance) can be brushed under the rug. As in the earlier figure, the latency versus time is often times piecewise constant (plus noise). Our toy example thus can be the same toy example from before, but with an added edge. Essentially, we want the lone negative curvature edge to \"appear\" when a different edge drops out. In particular, we can add a link from F to G that later goes down.\n",
    "\n",
    "We do also want around 5% of the edges to appear/disappear. With the small size of the old toy example, this agrees with only allowing one edge to pop in/out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d89b9b29506d1129e078cbafd5718f53824d76c4e79258120e482e59221da99"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
